{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6349bb6-f368-4038-869e-0150a1e8f9a6",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Jacky-lim-data-analyst/mini_project_cv/blob/main/w6_tutorial.ipynb)\n",
    "\n",
    "# Learning outcomes\n",
    "1. Image thresholding techniques\n",
    "    * Otsu thresholding\n",
    "2. Convolution operation in image\n",
    "    * Sharpening\n",
    "    * Blurring\n",
    "3. Edge detection\n",
    "4. Contour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397d9a2-2756-43b6-b463-59304f9ce5b5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfcf7f-c78a-4fc7-b243-51387c428d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 8)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "if cv.useOptimized():\n",
    "    cv.setUseOptimized(True)\n",
    "\n",
    "cv.useOptimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0a18a-4ceb-4537-9d34-5480675a117d",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    "Image thresholding is used to separate an image into foreground and background regions. It involves selecting a threshold value that partitions the image pixels into 2 classes. For example, those above the threshold are considered foreground, while those below are considered background.\n",
    "\n",
    "## Global thresholding\n",
    "Select a single threshold value based on the overall image histogram. The function is as shown:\n",
    "```{python}\n",
    "cv.threshold(src, thresh, maxval, type)\n",
    "```\n",
    "1. `src`: input image\n",
    "2. `thresh`: threshold value (pixel intensity)\n",
    "3. `maxval`: maximum value to be use with binary thresholding\n",
    "4. `type`: threshold flags.\n",
    "\n",
    "You can refer to the [online documentation](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a147222a96556ebc1d948b372bcd7ac59) for more info about the threshold flags.\n",
    "\n",
    "## Applications of image thresholding\n",
    "1. Image segmentation: separate objects of interest from background.\n",
    "2. Document image analysis: Binarize scanned document for Optical character recognition (OCR)\n",
    "3. Defect detection in manufactured parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bff78-cdff-4284-a8b7-211768b2e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the flags of thresholding mode\n",
    "flags = [i for i in dir(cv) if i.startswith(\"THRESH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cb2aa-2bb5-4f43-9fa2-50c47af81afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf00b1e-fc1f-4ead-b05f-f66cec1e3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.fromfunction(lambda i, j: j, (50, 256), dtype=np.uint8)\n",
    "\n",
    "def trackbar(x):\n",
    "    _, img1 = cv.threshold(img, x, 255, cv.THRESH_BINARY)\n",
    "    _, img2 = cv.threshold(img, x, 255, cv.THRESH_BINARY_INV)\n",
    "    _, img3 = cv.threshold(img, x, 255, cv.THRESH_TRUNC)\n",
    "    _, img4 = cv.threshold(img, x, 255, cv.THRESH_TOZERO)\n",
    "    _, img5 = cv.threshold(img, x, 255, cv.THRESH_TOZERO_INV)\n",
    "\n",
    "    cv.imshow('window', np.vstack([img1, img2, img3, img4, img5]))\n",
    "\n",
    "cv.namedWindow('window')\n",
    "cv.createTrackbar('threshold', 'window', 100, 255, trackbar)\n",
    "\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475da1c-2b5e-4760-84b9-b97bcbb0b931",
   "metadata": {},
   "source": [
    "### Otsu binarization\n",
    "In global thresholding, we used an arbitrary chosen value as a threshold. In contrast, Otsu's method avoids having to choose a value and determines it automatically. \n",
    "\n",
    "Consider an image with only 2 distinct image values (*bimodal image*), where the histogram would only consists of 2 peaks. A good threshold would be to select value in the middle of those 2 values. Similarly, Otsu's method determines an optimal global threshold value from the histogram.\n",
    "\n",
    "In order to do so, the `cv.threshold()` function is used, where `cv.THRESH_OTSU` is passed as an extra flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba9706-08c1-4d64-8077-3822c5cf41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04373a29-0465-4cee-a36d-1438d295d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv.imread(\"images/noisy.jfif\", 0)\n",
    "\n",
    "# global threshold\n",
    "ret, th1 = cv.threshold(gray, 127, 255, cv.THRESH_BINARY)\n",
    "\n",
    "# Otsu threshold\n",
    "ret, th2 = cv.threshold(gray, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\n",
    "\n",
    "# Gaussian blur + Otsu threshold\n",
    "blur = cv.GaussianBlur(gray, (5, 5), 0)\n",
    "ret, th3 = cv.threshold(blur, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\n",
    "\n",
    "# Plot grayscale images, their histogram and the resulting thresholded image\n",
    "images = [gray, None, th1,\n",
    "         gray, None, th2,\n",
    "         blur, None, th3]\n",
    "\n",
    "titles = [\"Original noisy image\", \"histogram\", \"global threshold\",\n",
    "         \"Original noisy image\", \"histogram\", \"Otsu threshold\",\n",
    "         \"Smoothed image\", \"histogram\", \"Otsu threshold\"]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, 3*i + 1), plt.imshow(images[3*i], cmap=\"gray\", vmin=0, vmax=255)\n",
    "    plt.title(titles[3*i]), plt.axis(\"off\")\n",
    "    plt.subplot(3, 3, 3*i + 2), plt.hist(images[3*i].ravel(), 256)\n",
    "    plt.title(titles[3*i+1])\n",
    "    plt.subplot(3, 3, 3*i + 3), plt.imshow(images[3*i + 2], cmap=\"gray\", vmin=0, vmax=255)\n",
    "    plt.title(titles[3*i+2]), plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5b143-4228-4d6f-98a8-e0a78e04a238",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "* Can you segment the leaf from the image \"leaf.jpg\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d20c56-5c25-4ea2-aa09-7e49ea5fb5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ea747c4-bbde-4c97-ba73-18b7795dac0c",
   "metadata": {},
   "source": [
    "# Introduction of convolution in image processing\n",
    "In image processing, a convolution kernel is a 2D matrix that is used to filter images. A convolution kernel is typically a square matrix, $M \\times N$, where both _M_ and _N_ are odd integers. An example of averaging kernel:  \n",
    "$kernel = \\frac{1}{9}\\begin {bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "\\end {bmatrix}$  \n",
    "A $ 3 \\times 3 $ 2D convolution kernel.\n",
    "\n",
    "Such kernel can be used to perform mathematical operations on each pixel of an image to achieve a desired effect (like blurring or sharpening an image). Check out this [video](https://www.youtube.com/watch?v=8rrHTtUzyZA) for more interactive examples and elaborations on the concepts of convolution.\n",
    "\n",
    "But why would you want to blur an image? Here are 2 reasons:\n",
    "* Because it reduces noise in an image.\n",
    "* To remove a distinct background.\n",
    "\n",
    "The value of individual elements in a kernel dictate the nature of filtering. For example, by changing the value of the kernel elements, you can also achieve sharpening effect.\n",
    "\n",
    "## Convolution in action\n",
    "![convolution](img_embed/2D_Convolution_Animation.gif \"Source: https://commons.wikimedia.org/wiki/File:2D_Convolution_Animation.gif\")\n",
    "\n",
    "## 2D convolution\n",
    "As in one-dimensional signals, images also can be filtered with various low-pass filters (LPF), high-pass filters (HPF) and etc. LPF helps in removing noise, blurring images, etc. HPF filters help in finding edges in images, which is also equivalent to original image minus the low-pass filtered image.\n",
    "\n",
    "OpenCV provides a function `cv.filter2D()` to convolve a kernel with an image. \n",
    "```python\n",
    "cv.filter2D(src, ddepth, kernel, ...)\n",
    "```\n",
    "1. The first argument, src is source image.\n",
    "2. ddepth indicates the depth of the resulting image. We will normally set it as -1 so that the final image will have the same depth as the source image, src.\n",
    "3. kernel is the convolution matrix\n",
    "4. To know more about optional argument `borderType`, please refer to this [link](https://docs.opencv.org/4.5.5/d2/de8/group__core__array.html#ga209f2f4869e304c82d07739337eae7c5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be24a8-be20-4f2f-9cf4-19c71947f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity kernel\n",
    "img = cv.imread('images/camera.jpg')\n",
    "\n",
    "kernel = np.zeros((3, 3), dtype=np.uint8)\n",
    "kernel[1, 1] = 1\n",
    "\n",
    "identity = cv.filter2D(img, -1, kernel)\n",
    "\n",
    "display_images([img, identity], (\"original\", \"identity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447b970-8bb3-4a49-9311-22441b499ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blurring with mean filtering\n",
    "kernel = np.ones((5, 5), dtype=np.float32) / 25\n",
    "dst = cv.filter2D(img, -1, kernel)\n",
    "\n",
    "display_images([img, dst], (\"original\", \"blurred\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3cfde-4b5e-401f-bfc2-641d97a74134",
   "metadata": {},
   "source": [
    "### Other function to smooth images\n",
    "Image blurring is achieved by convolving the image with a low-pass filter kernel. It is useful for removing noise, yet it also removes high frequency content (e.g. noise, edges) from the image. So edges are blurred a little bit in this operation (there are also blurring techniques which don't blur the edges). OpenCV provides four main types of blurring techniques.\n",
    "\n",
    "1. Averaging. This is done by the function `cv.blur()` or `cv.boxFilter()`. We will use `cv.blur()` in the following example.\n",
    "```python\n",
    "cv.blur(src, ksize)\n",
    "```\n",
    "1. The first argument is the source image.\n",
    "2. The second argument is the blurring kernel size. ($width, height$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329d7a2-a23a-4eac-a8cd-e7d6ea2820f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv.blur(img, (5, 5))\n",
    "\n",
    "display_images([img, blur], (\"original\", \"blur\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd5dfc-ba58-48f1-9b6b-c5edfed58fcc",
   "metadata": {},
   "source": [
    "2. Gaussian Blurring. This is done by `cv.GaussianBlur()`. We should specify the width and height of the kernel which should be positive and odd. We should also specify the standard deviation in the X and Y directions, sigmaX and sigmaY respectively. Better at preserving edges, but slower compared to its average counterparts\n",
    "```python\n",
    "cv.GaussianBlur(src, ksize, sigmaX, sigmaY, ...)\n",
    "```\n",
    "\n",
    "- First argument, src is the source image.\n",
    "- ksize is the Gaussian kernel size ($width, height$)\n",
    "- sigmaX is the Gaussian kernel standard deviation in X direction. \n",
    "- sigmaY is the Gaussian kernel standard deviation in Y direction. If sigmaY is set to zero, it is set to be equal to sigmaX. If both sigmaX and sigmaY are zero, then they are computed from ksize.width and ksize.height. Please refer to the [online documentation](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) for more details.\n",
    "\n",
    "3. Median Blurring. This is done by `cv.medianBlur()`. Best in removing salt and pepper noise. \n",
    "\n",
    "4. Bilateral filtering. This is done by `cv.bilateralFilter()`. It is highly effective in noise removal while keeping edges sharp. But the operation is slower compared to other filters. \n",
    "    - Bilateral filtering takes a Gaussian filter in space, but one more Gaussian filter which is a function of pixel difference. The Gaussian function of space makes sure that only nearby pixels are considered for blurring, while Gaussian function of intensity difference ensures that only those pixels with similar intensities are considered for blurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419dfee3-5ff0-4d39-b08a-d352c10e3251",
   "metadata": {},
   "source": [
    "### Image sharpening by convolutional kernel\n",
    "The sharpening kernel emphasizes differences in adjacent pixel values, making the whole image looks more vivid. The algorithm for sharpening an image is:\n",
    "1. Blur an image using arbitrary kernel you wish.\n",
    "2. Subtract the blurred image from the original image to get high frequency details. \n",
    "3. Add the high-frequency details to the original image.\n",
    "\n",
    "Source: [Blog post](https://blog.demofox.org/2022/02/26/image-sharpening-convolution-kernels/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1e808-4067-4bde-8b3d-e2fd1d96db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.array([[0, -1, 0],\n",
    "                  [-1, 5, -1],\n",
    "                  [0, -1, 0]])\n",
    "\n",
    "img = cv.imread('images/meal.jpg')\n",
    "sharp_img = cv.filter2D(img, -1, kernel)\n",
    "\n",
    "display_images([img, sharp_img], (\"original\", \"sharpen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc315f-7b5d-4329-8b45-ec2ce2e1d716",
   "metadata": {},
   "source": [
    "## Emboss filter\n",
    "The emboss filter is a popular image processing technique that generates a 3D, raised effect by manipulating pixel values based on local contrast and brightness differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bda12-1037-4ce2-97a1-1e5cefdfe1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_images\n",
    "\n",
    "kernel = np.array([[-2, -1, 0],\n",
    "                  [-1, 1, 1],\n",
    "                  [0, 1, 2]])\n",
    "\n",
    "img = cv.imread(\"images/car.jpg\")\n",
    "emboss_img = cv.filter2D(img, -1, kernel)\n",
    "display_images([img, emboss_img], (\"original\", \"emboss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64067a-2a61-4f4d-bc7f-6bd8aaed562a",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Apply average kernel with increasing size on an image. Use \"dog.jfif\" for this exercise. What can you infer from the output images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fe851-9120-4a12-ba82-dd41e90f776c",
   "metadata": {},
   "source": [
    "# Edge detection\n",
    "Consider an image, how can we find the salient edges? Qualitatively, edges occur at boundaries between regions of different color, intensity or texture. Often, it is preferable to detect edges using only purely local information.\n",
    "\n",
    "Edges are characterized by sudden change in pixel intensity. To detect edges, we need to look for such changes in the neighboring pixels. You can easily notice that in an edge the pixel intensity changes in a notorious way. A good way to express changes is by using derivatives. A high change in gradient indicates a major change in the image. \n",
    "\n",
    "The method to detect edges can be performed by locating pixel locations where the gradient is higher than its neighbors (or to generalize, higher than a threshold). A Sobel operator is a discrete differentiation operator. It computes an approximation of the gradient of an image intensity function. The Sobel operator combines Gaussian smoothing and differentiation.\n",
    "\n",
    "## Image Gradients\n",
    "\n",
    "OpenCV provides 3 types of gradient filters or HPF: Sobel, Scharr and Laplacian. \n",
    "\n",
    "1. Sobel and Scharr Derivatives  \n",
    "Sobel operators is a joint Gaussian smoothing plus differentiation operation, so it is more resistant to noise. You can specify the direction of derivatives to be taken, vertical or horizontal (by the arguments, yorder and xorder respectively). When applying Sobel operator for edge detection, we need to convert the color image to grayscale image. If you want to apply Scharr derivatives, you can set `ksize` to be -1.\n",
    "\n",
    "$$Sobel_x = \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$Sobel_y = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix}$$\n",
    "\n",
    "$$Scharr_x = \\begin{bmatrix} 3 & 0 & -3 \\\\ 10 & 0 & -10 \\\\ 3 & 0 & -3 \\end{bmatrix}$$\n",
    "\n",
    "$$Scharr_y = \\begin{bmatrix} 3 & 10 & 3 \\\\ 0 & 0 & 0 \\\\ -3 & -10 & -3 \\end{bmatrix}$$\n",
    "\n",
    "3. Laplacian Derivatives. One such example is as shown below:\n",
    "$$kernel= \\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb256c6-831d-4c04-90db-eb730dac2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images/wood_planck.jfif\")\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, img_wood = cv.threshold(gray, 200, 255, cv.THRESH_BINARY_INV)\n",
    "\n",
    "# output dtype = cv.CV_8U\n",
    "# detect horizontal edges\n",
    "sobely8u = cv.Sobel(img_wood, cv.CV_8U, 0, 1, ksize=5)\n",
    "\n",
    "# set output type as cv.CV_64F\n",
    "sobely64f = cv.Sobel(img_wood, cv.CV_64F, 0, 1, ksize=5)\n",
    "sobely_8u = cv.convertScaleAbs(sobely64f)\n",
    "\n",
    "from utils import matplotlib_show_images\n",
    "matplotlib_show_images([img_wood, sobely8u, sobely_8u], 1, 3, \n",
    "                      titles=[\"Original\", \"Sobel CV_8U\", \"Sobel abs CV_8U\"],\n",
    "                      figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aafcca-6376-46e5-a6b8-4da3087c4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images/camera.jpg\", 0)\n",
    "\n",
    "laplacian = cv.Laplacian(img, cv.CV_64F, ksize=3)\n",
    "laplacian = cv.convertScaleAbs(laplacian)\n",
    "sobelx = cv.Sobel(img, cv.CV_64F, 1, 0, ksize=3)\n",
    "sobelx = cv.convertScaleAbs(sobelx)\n",
    "sobely = cv.Sobel(img, cv.CV_64F, 0, 1, ksize=3)\n",
    "sobely = cv.convertScaleAbs(sobely)\n",
    "\n",
    "from utils import display_images\n",
    "\n",
    "display_images([laplacian, sobelx, sobely], \n",
    "               (\"laplacian\", \"Sobel x\", \"Sobel y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038352d1-2644-4dbc-9b9f-71b13caa55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "laplacian_gaussian = cv.Laplacian(blur, cv.CV_64F, ksize=3)\n",
    "laplacian_gaussian = cv.convertScaleAbs(laplacian_gaussian)\n",
    "\n",
    "display_images([laplacian, laplacian_gaussian], (\"laplacian\", \"laplacian blur\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ed506-3955-4159-8cf4-92cfb80ac308",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Use convolution `cv.filter2D()` to implement Laplacian filter on 'lena.jfif'.\n",
    "2. What happen if you add the results of Laplacian of Gaussian (LOG) filters into the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860173e-37cf-4612-b996-07dfea9d4320",
   "metadata": {},
   "source": [
    "## Canny Edge Detection\n",
    "\n",
    "1. **Noise Reduction**  \n",
    "Since edge detection is susceptible to noise in the image, the first step is to remove noise using 5x5 Gaussian filter.\n",
    "\n",
    "2. **Finding Intensity Gradient of the image**. Sobel operators x and y are utilized.\n",
    "\n",
    "3. **Non-maximum Suppression** is to filter out unwanted pixels (which do not constitute valid edges). To accomplish that, each pixel is compared to its neighboring pixels, in the positive and negative gradient direction. If the gradient magnitude of the current pixel is greater than its neighboring pixels, it is left unchanged. Otherwise, the magnitude of the current pixel is set to zero. \n",
    "\n",
    "4. **Hysterisis thresholding**. The gradient magnitude are compared with the 2 threshold values.\n",
    "\n",
    "### Canny edge detection in OpenCV\n",
    "OpenCV puts all the above in single function, `cv.Canny()`. \n",
    "```python\n",
    "# The 4th and 5th arguments are optional.\n",
    "cv.Canny(img, threshold1, threshold2, apertureSize=3, L2gradient = False)\n",
    "```\n",
    "* First argument is input image.\n",
    "* Second and third arguments are minVal and maxVal respectively.\n",
    "* Fouth argument is aperture size, which is the size of Sobel operator used to find gradients. Default is 3.\n",
    "* Last argument is L2 gradient which specifies the equation to find the gradient magnitude. It can be `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a75dd5-83e9-4f74-a642-a20f564280da",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images/camera.jpg\", 0)\n",
    "\n",
    "edges = cv.Canny(img, 100, 300)\n",
    "display_images([img, edges], (\"original\", \"edges map\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee6aef-35ed-41de-bf4b-32dee18fb7fd",
   "metadata": {},
   "source": [
    "Generally speaking, Canny edge detection produces the best results in most applications because it provides more flexibility in how edges are identified and connected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77368c-84ba-491c-ace5-18efec4f901b",
   "metadata": {},
   "source": [
    "### Zero-parameter / automatic Canny edge detector\n",
    "From the examples above, we know that `threshold1` and `threshold2` are two of the most crucial parameters to control the quality of edge detection results. This begs a question: Can we automate Canny edge detection by selecting the 2 parameters algorithmically? The answer is yes.\n",
    "\n",
    "The example code on the implementation will be shown in the class.\n",
    "\n",
    "---\n",
    "\n",
    "**Further reading**\n",
    "\n",
    "For more information on the determination of threshold range, please refer to this [pyimagesearch](https://pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/) and [stackabuse](https://stackabuse.com/opencv-edge-detection-in-python-with-cv2canny/) posts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0679954-4440-4a87-ac3c-d9013df9ca40",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Create a trackbar that control the hysterisis thresholds and display the resulting images from the changes in the thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9cb9f-db8a-4583-b5e0-b0206c66b998",
   "metadata": {},
   "source": [
    "# What are contours?\n",
    "While isolated edges can be used for some applications, like line detection, they become even more useful when linked into continuous contours.\n",
    "\n",
    "Contours can be explained simply as curve joining all the continuous points (along the boundary), having the same color and intensity. The contours are a useful tool for shape analysis as well as object detection and recognition. As such, it is often one of the most important steps for numerous applications, such as *image segmentation*. \n",
    "\n",
    "When we join all the points on the boundary of an object, we get a contour.\n",
    "\n",
    "Some suggestions in the OpenCV implementation:\n",
    "* For better accuracy, use binary images. So, before finding contours, apply threshold or canny edge detection.\n",
    "* In OpenCV, finding contours is like finding white object from black background. So, object to be found should be white and background should be black.\n",
    "\n",
    "## Find contours\n",
    "This is done through function `cv.findContours()`.\n",
    "* First argument is source image, which should be 8-bit single channel image. The image is treated as binary image.\n",
    "* Second argument is contour retrieval mode. Refer to [online documentation](https://docs.opencv.org/3.4/d3/dc0/group__imgproc__shape.html#ga819779b9857cc2f8601e6526a3a5bc71) for more info.\n",
    "    * cv.RETR_EXTERNAL\n",
    "    * cv.RETR_LIST\n",
    "    * cv.RETR_CCOMP\n",
    "    * cv.RETR_TREE\n",
    "- Third argument is contour approximation method. Refer to [online documentation](https://docs.opencv.org/3.4/d3/dc0/group__imgproc__shape.html#ga4303f45752694956374734a03c54d5ff) for more info.\n",
    "    * cv.CHAIN_APPROX_NONE\n",
    "    * cv.CHAIN_APPROX_SIMPLE\n",
    "    * cv.APPROX_TC89_L1\n",
    "    * cv.APPROX_TC89_KCOS\n",
    "- Outputs: 1. contours and 2. hierarchy\n",
    "\n",
    "## Draw contours\n",
    "To draw contours, `cv.drawContours()` function is used.\n",
    "- First argument is source image\n",
    "- Second argument is the contours (Python list)\n",
    "- Third argument is the index of contours (useful when drawing individual contour). To draw all contours, pass -1.\n",
    "- Remaining arguments are color, thickness and etc.\n",
    "\n",
    "```{python}\n",
    "# To draw all the contours in an image\n",
    "cv.drawContours(img, contours, -1, (0, 255, 0), 3)\n",
    "# To draw an individual contour, say 4th contour\n",
    "cv.drawContours(img, contours, 3, (0, 255, 0), 3)\n",
    "# or\n",
    "cnt = contours[4]\n",
    "cv.drawContours(img, [cnt], 0, (0, 255, 0), 3)\n",
    "```\n",
    "\n",
    "## Steps for detecting and drawing contours in OpenCV\n",
    "1. Read the image in grayscale format.\n",
    "2. Apply binary thresholding or Canny edge detection on the image.\n",
    "3. Find the contours from the resulting image in (2).\n",
    "4. Draw contours on original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903d74e-63ae-4ffd-8df5-979ec8f497b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((256, 256), dtype=np.uint8)\n",
    "\n",
    "cv.rectangle(img, (25, 25), (231, 231), 255, -1)\n",
    "\n",
    "contours, hierarchy = cv.findContours(img, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff0ff4-fafb-4dfd-802c-845a3f351efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"How many set of contours was found? {len(contours)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ebb10-7d36-41ba-978c-7d4de2ba88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996a11e-0a05-4592-9660-88825300653a",
   "metadata": {},
   "source": [
    "It turns out that the contour is made up of 4 sets of corner coordinates. Lets draw the contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569cff5-cdf4-4c7f-876c-35a7da4ed23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = contours[0]\n",
    "img_bgr = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "cv.drawContours(img_bgr, [cnt], 0, (0, 255, 0), 3)\n",
    "\n",
    "from utils import display_image\n",
    "display_image(\"contours\", img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d593cc-5a86-4e3f-b8ba-1ab2faa06ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: grayscale image\n",
    "img = cv.imread('images/monitor.jfif')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "#2: binary thresholding\n",
    "_, th = cv.threshold(gray, 200, 255, cv.THRESH_BINARY_INV)\n",
    "\n",
    "display_images([img, th], (\"original\", \"threshold\"))\n",
    "\n",
    "# find and draw contours\n",
    "contours, hierarchy = cv.findContours(th, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)\n",
    "img_copy = img.copy()\n",
    "cv.drawContours(img_copy, contours, -1, (0, 255, 0), 1, cv.LINE_AA)\n",
    "\n",
    "display_image(\"contours\", img_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d92c13-c290-42dd-a5f3-03bb798b6aa9",
   "metadata": {},
   "source": [
    "Try `cv.RETR_EXTERNAL` to retrieve just the external boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25039c-7312-4cb5-b710-7396327f4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv.findContours(th, cv.RETR_EXTERNAL, \n",
    "                                     cv.CHAIN_APPROX_SIMPLE)\n",
    "img_copy = img.copy()\n",
    "cv.drawContours(img_copy, contours, -1, (0, 255, 0), 1, cv.LINE_AA)\n",
    "\n",
    "display_image(\"contours\", img_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee7c84-a43e-44c2-9153-54fad8735ed2",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Can you think of a way to filter out just the monitor contour by using `cv.RETR_LIST` or `cv.RETR_TREE`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0678ff-b27d-45e5-b133-b5a20d02ee00",
   "metadata": {},
   "source": [
    "## Contour features\n",
    "### 1. Moments\n",
    "Image moments help you to calculate some features like center of mass of the object, area of object etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ea9e4-8bc0-4ca6-b15d-e712e00f86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = contours[0]\n",
    "M = cv.moments(cnt)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df1530-9e2d-40ac-9ac5-3a1e808e28d3",
   "metadata": {},
   "source": [
    "### 2. Centroid, area and perimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f723cc7-1e79-4498-854c-ea51f942db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = int(M['m10'] / M['m00'])\n",
    "cy = int(M['m01'] / M['m00'])\n",
    "\n",
    "print(\"The centroid: {}\".format((cx, cy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621582d-5730-46f4-aaf4-314e20ad1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"area: {cv.contourArea(cnt):.3f}\")\n",
    "print(f\"perimeter: {cv.arcLength(cnt, True):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a9d24-6066-4ecd-bfaf-29e6b62183a3",
   "metadata": {},
   "source": [
    "### 3. Contour approximation\n",
    "It approximates a contour shape to another shape with less number of vertices depending upon the precision we specify.\n",
    "```{python}\n",
    "eps = 0.1*cv.arcLength(cnt, True)\n",
    "approx = cv.approxPolyDP(cnt, eps, True)\n",
    "```\n",
    "A wise selection of eps is needed to get the correct output.\n",
    "\n",
    "### 4. Bounding rectangle\n",
    "\n",
    "#### Straight bounding rectangle\n",
    "```{python}\n",
    "x, y, w, h = cv.boundingRect(cnt)\n",
    "cv.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "```\n",
    "\n",
    "#### Rotated rectangle\n",
    "Here the bounding rectangle is drawn with a minimum area. The function is `cv.minAreaRect()`. It returns a Box 2D structure which contains centers, $(x, y)$, dimension, $(width, height)$ and angle.\n",
    "```{python}\n",
    "rect = cv.minAreaRect(cnt)\n",
    "box = cv.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "cv.drawContours(img, [box], 0, (0, 0, 255), 2)\n",
    "```\n",
    "\n",
    "### 5. Minimum enclosing circle\n",
    "```{python}\n",
    "(x, y), radius = cv.minEnclosingCircle(cnt)\n",
    "center = (int(x), int(y))\n",
    "radius = int(radius)\n",
    "cv.circle(img, center, radius, (0, 0, 255), 2)\n",
    "```\n",
    "\n",
    "### 6. Fitting an ellipse\n",
    "```{python}\n",
    "ellipse = cv.fitEllipse(cnt)\n",
    "cv.ellipse(img, ellipse, (0, 255, 0), 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118ca03-9e6b-4f95-8fe8-d566baaccebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('images/car.jfif', 0)\n",
    "\n",
    "ret, thresh = cv.threshold(img, 200, 255, cv.THRESH_BINARY_INV)\n",
    "contours, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "print(len(contours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38054789-5051-4e61-9226-cec2f9cb5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw contours\n",
    "cnt = contours[0]\n",
    "img_bgr = cv.imread('images/car.jfif')\n",
    "img_bgr_copy = img_bgr.copy()\n",
    "cv.drawContours(img_bgr_copy, [cnt], 0, (0, 0, 255), 2, cv.LINE_AA)\n",
    "\n",
    "display_image(\"contour\", img_bgr_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fefe19-6f4e-486f-a95d-595d79654b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = cv.boundingRect(cnt)\n",
    "cv.rectangle(img_bgr, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "display_image(\"bounding box\", img_bgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ec090-d4bb-4f3f-bfc1-72a4967ac692",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "* Draw\n",
    "    * Bounding box\n",
    "    * Rotated bounding box\n",
    "    * Minimum enclosing circle\n",
    "\n",
    "on the bird in image \"bird.jpg\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643fd39-50f2-4ef2-afe1-e0335e6788c5",
   "metadata": {},
   "source": [
    "# Weekly activities\n",
    "1. Apply custom sharpening kernel of aperture size 3 and 5 as shown below on 'native-bee.png':  \n",
    "$ 3 \\times 3$ kernel:  \n",
    "$ \\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\\\\\n",
    "\\end{bmatrix}$  \n",
    "$ 5 \\times 5$ kernel:  \n",
    "$ \\begin{bmatrix}\n",
    "-1 & -1 & -1 & -1 & -1 \\\\\n",
    "-1 & -1 & -1 & -1 & -1 \\\\\n",
    "-1 & -1 & 25 & -1 & -1 \\\\\n",
    "-1 & -1 & -1 & -1 & -1 \\\\\n",
    "-1 & -1 & -1 & -1 & -1 \\\\\n",
    "\\end{bmatrix}$  \n",
    "What can you infer from the outputs?\n",
    "2. Apply different image smoothing techniques (e.g. average filter, Gaussian kernel and median filter) on 'noise_lena.jpg' and display the resulting images after the convolution. Comment on the outcomes and deduce the type of noise present on the image.\n",
    "3. Write a program to *segment the boat and the people on it from the background*. Follow the instruction below:\n",
    "    - Use 'boat.jpg' as input.\n",
    "    - Apply Otsu thresholding.\n",
    "    - Draw bounding box to identify the region where the boat and people are located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc90fc-a121-4326-81f2-7196afadebc8",
   "metadata": {},
   "source": [
    "# Useful links:\n",
    "- Learn more about different types of image filters: https://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d9fca-85e6-4787-98f3-70f8c9ebd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "#Using 3x3 kernel\n",
    "kernel3x3 = np.array([[0, -1, 0],\n",
    "                  [-1, 5, -1],\n",
    "                  [0, -1, 0]])\n",
    "\n",
    "#Using 5x5 kernel\n",
    "kernel5x5 = np.array([[-1, -1, -1, -1, -1],\n",
    "                    [-1, -1, -1, -1, -1],\n",
    "                    [-1, -1, 25, -1, -1],\n",
    "                    [-1, -1, -1, -1, -1],\n",
    "                    [-1, -1, -1, -1, -1]])\n",
    "\n",
    "img = cv.imread('images/native-bee.jpg')\n",
    "sharp_img3x3 = cv.filter2D(img, -1, kernel3x3)\n",
    "sharp_img5x5 = cv.filter2D(img, -1, kernel5x5)\n",
    "\n",
    "\n",
    "display_images([sharp_img3x3, sharp_img5x5], (\"Kernel of aperture size 3\", \"Kernel of aperture size 5\"))\n",
    "#Using a 3x3 filter enhances the edges of an image, making details more noticeable. \n",
    "#In contrast, a 5x5 filter creates a much stronger sharpening effect due to its higher central value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ec4ef-ff63-4b39-8865-468ce5da0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\n",
    "# Load the image\n",
    "img = cv.imread('images/noise_lene.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply an average filter\n",
    "average_filtered = cv.blur(img, (5, 5))\n",
    "\n",
    "# Apply a Gaussian filter\n",
    "gaussian_filtered = cv.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "# Apply a median filter\n",
    "median_filtered = cv.medianBlur(img, 5)\n",
    "\n",
    "display_images([img, average_filtered, gaussian_filtered, median_filtered], ['Original', 'Average Filter', 'Gaussian Filter', 'Median Filter'])\n",
    "\n",
    "#Average Filter: This filter smooths the image by averaging the pixel values in a neighborhood. It works well for reducing random noise but may blur edges.\n",
    "#Gaussian Filter: This filter uses a Gaussian kernel to perform smoothing. It is effective in reducing Gaussian noise and preserves edges better than the average filter.\n",
    "#Median Filter: This filter replaces each pixel’s value with the median value of the neighboring pixels. It is particularly effective at removing salt-and-pepper noise while preserving edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318d20b-6318-4009-9e81-72854359d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "# Load the image\n",
    "image = cv.imread('images/boat.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply Otsu's thresholding\n",
    "_, otsu_thresh = cv.threshold(image, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv.findContours(otsu_thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw bounding boxes\n",
    "image_color = cv.cvtColor(image, cv.COLOR_GRAY2BGR)  # Convert grayscale to BGR for colored bounding boxes\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv.boundingRect(contour)\n",
    "    cv2.rectangle(image_color, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "display_images([boat_image, otsu_thresh, output_image], ['Original', 'Otsu Thresholding', 'Segmented with Bounding Boxes'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
