{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dbf85d-96f7-465f-89bc-6970752b8cd6",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Jacky-lim-data-analyst/mini_project_cv/blob/main/w5_tutorial.ipynb)\n",
    "\n",
    "# Learning outcomes\n",
    "1. Enhance brightness and contrast of images: gamma correction and histogram equalization\n",
    "2. Drawing / Annotatitions on images\n",
    "3. Image transformation (Image augmentation)\n",
    "4. More image augmentation techniques\n",
    "5. Bitwise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735c20c-d118-4129-8677-a8b9c58a93ab",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78196f2-8118-4c00-8a9d-227a633c2187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 8)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "if not cv.useOptimized():\n",
    "    cv.setUseOptimized(True)\n",
    "\n",
    "cv.useOptimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ac318-61aa-4797-8adb-80e15a52fc39",
   "metadata": {},
   "source": [
    "# Enhance brightness and contrast of images\n",
    "\n",
    "## Gamma correction\n",
    "\n",
    "*Our eyes do not perceive light the way cameras do*. With a digital camera, when twice the number of photons hit the sensor, it receives twice the signal (a \"linear\" relationship). That's not how our eyes work. Instead, we perceive twice the light as being only a fraction brighter, and increasingly so for higher light intensities. \n",
    "Compared to a camera, we are much more sensitive to changes in dark tones than we are to similar changes in bright tones. \n",
    "So, how does all this relate to gamma? Gamma is what translates between our eye's light sensitivity and that of the camera.\n",
    "Gamma correction can be used to correct the brightness of an image by using a non-linear transformation between the input values and the mapped output values.\n",
    "$$O=(\\frac{I}{255})^{\\gamma}\\times255$$\n",
    "\n",
    "![note_gamma](img_embed/gamma_correction.jpg \"gamma correction plot\")\n",
    "\n",
    "Image credit from [OpenCV official documentation](https://docs.opencv.org/3.4/d3/dc1/tutorial_basic_linear_transform.html).\n",
    "\n",
    "Most image files use an encoding gamma of 1/2.2. More info on gamma correction can be found in this [webpage](https://www.cambridgeincolour.com/tutorials/gamma-correction.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d582e-01b9-49fb-ad83-3e8476ed6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images 3/alley_night.jpg\")\n",
    "\n",
    "# gamma correction\n",
    "#1: create a lookup table\n",
    "lookUpTable = np.empty((1, 256), dtype=np.uint8)\n",
    "gamma = 0.5  # less than 1: enhance, more than 1: reduce\n",
    "\n",
    "for i in range(256):\n",
    "    lookUpTable[0, i] = np.clip(pow(i / 255.0, gamma) * 255, 0, 255)\n",
    "\n",
    "res = cv.LUT(img, lookUpTable)\n",
    "\n",
    "from utils import display_images\n",
    "\n",
    "display_images([img, res], (\"original\", f\"gamma={gamma}\")) #image look brighter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81a4c7-07a6-40bf-b7b9-ae2b12ce4bac",
   "metadata": {},
   "source": [
    "# Introduction of image histogram\n",
    "In the realm of image processing, histogram of an image shows the distribution of pixel intensity values. In other word, it is a graph showing the number of pixels in an image at each different intensity value, normally range from 0-255.\n",
    "\n",
    "The histogram of an image is a discrete function $h(r_k)$ that counts the number of pixels in the image with intensity level $r_k$.\n",
    "\n",
    "The function is as shown below:\n",
    "\n",
    "\n",
    "## Histogram equalization\n",
    "Histogram equalization is a technique that redistributes the pixel intensities to achieve a uniform histogram, thereby improving the **contrast** of an image.\n",
    "\n",
    "The function is as shown below:\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "For more info on the formula of histogram equalization, please refer to this [Wikipedia article](https://en.wikipedia.org/wiki/Histogram_equalization).\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of image histogram\n",
    "1. Contrast adjustment\n",
    "2. Thresholding: Histogram can help to determine appropriate thresholds for segmenting an image into foreground and background.\n",
    "3. Image retrieval: Histogram can be used as compact representation of image content.\n",
    "4. Image analysis: Provide valuable statistical info about the distribution of pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f169962-ed01-4e87-ad3c-ad82ebfb73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread(\"images 3/camera.jpg\")\n",
    "img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "hist = cv.calcHist([img_gray], [0], None, [256], [0, 256])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist)\n",
    "plt.title('grayscale image histogram')\n",
    "plt.xlabel(\"Bins\")\n",
    "plt.ylabel(\"# of pixels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99217a-5927-4d82-ac00-5e717b369f1d",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Draw histogram for each channel in an image array on the same plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa628b4-9171-4c46-a27b-6dffad92d3fc",
   "metadata": {},
   "source": [
    "**Histogram equalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62485f52-a3dc-4b93-9623-ae6d7b867851",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('images 3/low_contrast_img.jfif', 0)\n",
    "hist, bins = np.histogram(img.flatten(), 256, [0, 256])\n",
    "\n",
    "#cdf\n",
    "cdf = hist.cumsum()\n",
    "cdf_normalized = cdf*float(hist.max()) / cdf.max()\n",
    "\n",
    "plt.plot(cdf_normalized, color='b')\n",
    "plt.hist(img.flatten(), 256, [0, 256], color='r') #color can be r/g/b\n",
    "plt.xlim([0, 255])\n",
    "plt.legend(('cdf', 'histogram'), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67361492-0bee-4822-8b03-c74f5bf7285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_images\n",
    "dst = cv.equalizeHist(img)\n",
    "\n",
    "display_images([img, dst], (\"original\", \"histogram equalized\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c98fb3-5eb1-4426-9f00-aa8180d06456",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(dst.flatten(), 256, [0, 256])\n",
    "\n",
    "cdf = hist.cumsum()\n",
    "cdf_normalized = cdf*float(hist.max()) / cdf.max()\n",
    "\n",
    "plt.plot(cdf_normalized, color='b')\n",
    "plt.hist(dst.flatten(), 256, [0, 256], color='r')\n",
    "plt.xlim([0, 255])\n",
    "plt.legend(('cdf', 'histogram'), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea2a15-92d5-48c3-bb27-d3de501f97f5",
   "metadata": {},
   "source": [
    "## Contrast limited adaptive histogram equalization (CLAHE)\n",
    "In adaptive histogram equalization, image is divided into small blocks called \"tiles\" (tileSize is 8x8 by default in OpenCV). Then, each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above a specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization.\n",
    "\n",
    "---\n",
    "\n",
    "**Further reading**\n",
    "\n",
    "Refer to this [MATLAB official documentation](https://www.mathworks.com/help/visionhdl/ug/contrast-adaptive-histogram-equalization.html) for the algorithm of CLAHE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11d1ba-cc1a-47db-bbd5-3f24f3c5864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_images\n",
    "\n",
    "gray = cv.imread(\"images/winter_low_contrast.jfif\", 0)\n",
    "\n",
    "eq = cv.equalizeHist(gray)\n",
    "clahe = cv.createCLAHE(clipLimit=4, tileGridSize=(9, 9))\n",
    "dst = clahe.apply(gray)\n",
    "\n",
    "display_images([gray, eq, dst], (\"grayscale\", \"equalized\", \"CLAHE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ade798-6008-4c5b-a20a-0637d5b6c711",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Try histogram equalization and CLAHE on 'winter.jfif' and display the results together with the original grayscale image. Comment on the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478c33c-a73f-46f6-83c1-9da963b7e335",
   "metadata": {},
   "source": [
    "# Annotating images using OpenCV\n",
    "Why image annotations? Below are some of the use cases:\n",
    "- Adding information to your images\n",
    "- Drawing bounding boxes around objects\n",
    "\n",
    "## Draw line on image\n",
    "The associated function is `cv.line()`:\n",
    "```\n",
    "cv.line(img, pt1, pt2, color, thickness, lineType, shift)\n",
    "```\n",
    "1. First argument, img is the source image\n",
    "2. pt1 is the first point of the line segment.\n",
    "3. pt3 is the second point of the line segment.\n",
    "4. thickness is like what the name suggests.\n",
    "5. linetype is optional flags. Examples are:\n",
    "    - cv.FILLED\n",
    "    - cv.LINE_4\n",
    "    - cv.LINE_8\n",
    "    - cv.LINE_AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e7ae0-d310-407c-9945-b25acf74faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images 3/meal.jpg\")\n",
    "img_copy = img.copy()\n",
    "# img.shape  (614, 826)\n",
    "start_point = (100, 150)\n",
    "end_point = (100, 600)\n",
    "\n",
    "cv.line(img_copy, start_point, end_point, (100, 150, 50), 2, cv.LINE_AA)\n",
    "\n",
    "cv.imshow('line', img_copy)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f15a4-2ecd-46ff-b375-e2e2c74111ad",
   "metadata": {},
   "source": [
    "## Draw rectangle\n",
    "The function is `cv.rectangle()`.\n",
    "```python\n",
    "cv.rectangle(img, pt1, pt2, color, thickness, linetype, shift)\n",
    "```\n",
    "1. First argument, img is the source image.\n",
    "2. pt1 is the vertex / corner of rectangle.\n",
    "3. pt2 is the opposite vertex / corner of rectangle. Together, pt1 and pt2 form the diagonal of the rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd321852-3c47-45c4-9dd5-62df3d8a4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images 3/meal.jpg\")\n",
    "img_copy = img.copy()\n",
    "pts1 = (160, 250)\n",
    "pts2 = (485, 475)\n",
    "\n",
    "cv.rectangle(img_copy, pts1, pts2, (0, 0, 255), 3, cv.LINE_AA)\n",
    "\n",
    "cv.imshow(\"rectangle\", img_copy)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1477f6-2ca6-43f4-a618-6c137aa7683b",
   "metadata": {},
   "source": [
    "## Draw circle\n",
    "The function is `cv.circle()`.\n",
    "```python\n",
    "cv.ellipse(img, center, radius, color, thickness, linetype)\n",
    "```\n",
    "1. First argument, img is the source image.\n",
    "2. center is the center coordinates of ellipse.\n",
    "3. radius must be an `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb661497-bb2b-4a24-8d53-faca79f40fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images 3/meal.jpg\")\n",
    "img_copy = img.copy()\n",
    "center = (310, 340)\n",
    "radius = 100\n",
    "\n",
    "cv.circle(img_copy, center, radius, (255, 0, 0), 2, cv.LINE_AA)\n",
    "\n",
    "cv.imshow(\"rectangle\", img_copy)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d753d-6d54-43c2-9d9e-b5b2e083e5da",
   "metadata": {},
   "source": [
    "## Draw ellipse\n",
    "The function is `cv.ellipse()`.\n",
    "```python\n",
    "cv.ellipse(img, center, axes, angle, startAngle, endAngle, color, ...)\n",
    "```\n",
    "1. First argument, img is the source image.\n",
    "2. center is the center coordinates of ellipse.\n",
    "3. axes is a tuple of half the axes (major & minor) lengths.\n",
    "4. angle is the rotation angle. Both axes and angle arguments will determine if the output is a *horizontal* ellipse or a *vertical* ellipse.\n",
    "5. If we want to draw full ellipse, then set `startAngle = 0` and `endAngle = 360`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993641c-2116-4884-aa40-ec3f25c975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images 3/meal.jpg\")\n",
    "img_copy = img.copy()\n",
    "\n",
    "center = (310, 340)\n",
    "axes = (155, 95)\n",
    "\n",
    "cv.ellipse(img_copy, center, axes, 0, 0, 360, (0, 255, 0), 2) #if angle change to 180, become u shape\n",
    "cv.imshow(\"ellipse\", img_copy)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619d484-efab-4e1c-afba-d41bcf379e86",
   "metadata": {},
   "source": [
    "## Text on image\n",
    "```\n",
    "cv.putText()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a38e2c-61dd-43e5-af47-e8adf3602e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images 3/meal.jpg\")\n",
    "img_copy = img.copy()\n",
    "\n",
    "text = \"The food is delicious!\" #h, w = img.copy.shape[:2]\n",
    "org = (25, 25) #org = (int(w // 2 = 50),50)\n",
    "cv.putText(img_copy, text, org, cv.FONT_HERSHEY_SIMPLEX, 0.7, (200, 100, 30), 2)\n",
    "cv.imshow('text on image', img_copy)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa68fa-c092-4df8-9fb8-7609ac1e66c1",
   "metadata": {},
   "source": [
    "## Exercise: Drawing functions\n",
    "1. Draw a bulleye image like what shown below:  \n",
    "![bulleye](img_embed/bulleye.PNG \"bulleye\")\n",
    "2. Draw a finish line image as below:\n",
    "\n",
    "![finish_line](img_embed/custom_draw.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a38862-1821-4ce0-a200-568f0347d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: create a black grayscale image\n",
    "#2: create a radius list\n",
    "#3: loop through radius list to draw the circle\n",
    "#4: insert text\n",
    "canva = np.zeros((350,350), dtype = \"uint8\")\n",
    "h, w = canva.shape[:2]\n",
    "center = (w //2, h//2)\n",
    "\n",
    "#list of radius\n",
    "radius = np.linspace(10,160,num=10).astype(\"int32\")\n",
    "\n",
    "for r in radius:\n",
    "    cv.circle(canva, center, r, 255, 2)\n",
    "\n",
    "cv.putText(canva, \"Bulleye\", (20, 325), cv.FONT_HERSHEY_PLAIN, 1.2, 255)\n",
    "from utils import display_images\n",
    "display_images(\"bulleye\", canva)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca0b99-60ff-4f4c-af9e-9941caebba40",
   "metadata": {},
   "source": [
    "# Affine transformation operations\n",
    "Rotation and translation of images are among the most fundamental operations in image editing. Both fall under a broader class of *affine transformation*. Affine transformation is a linear mapping method that preserves points, straight lines and planes. Sets of parallel lines remain parallel after an affine transformation. Mathematically speaking, a transformation that can be expressed as the form of matrix multiplication (linear combination) followed by vector addition (translation).\n",
    "\n",
    "$$A = \\begin{bmatrix} a_{00} & a_{01} \\\\ a_{10} & a_{11} \\end{bmatrix}, \n",
    "B = \\begin{bmatrix} b_{0} \\\\ b_{1} \\end{bmatrix} $$\n",
    "\n",
    "$$ M = \\begin{bmatrix} A & B \\end{bmatrix} $$\n",
    "\n",
    "$$ T = A . \\begin{bmatrix} x \\\\ y \\end{bmatrix} + B $$\n",
    "\n",
    "$$ T = M . [x, y, 1]^T$$\n",
    "\n",
    "$$ T = \\begin{bmatrix} a_{00}x + a_{01}y + b_{0} \\\\ a_{10}x + a_{11}y + b_{1} \\end{bmatrix}$$\n",
    "\n",
    "The affine transformation is typically used to correct for geometric distortions or deformations caused by non-ideal camera angles.\n",
    "\n",
    "## Rotation \n",
    "Two key functions:\n",
    "- `cv.getRotationMatrix2D(center, angle, scale)`\n",
    "    1. First argument is the center of rotation (coordinates, which is expressed as tuple).\n",
    "    2. Second argument is angle of rotation in degrees.\n",
    "    3. scale: scale factor which scales the image up or down.\n",
    "- `cv.warpAffine(src, M, dsize, ...)`\n",
    "    1. First argument is the source image.\n",
    "    2. M is the transformation matrix (output of the `cv.getRotationMatrix2D()` function. \n",
    "    3. dsize is size of output image. \n",
    "... denotes optional arguments. For the complete list of arguments, visit the online [OpenCV documentation](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983)\n",
    "\n",
    "## Translation\n",
    "- Create transformation matrix, $M$ which is a 2x3 array.\n",
    "- Call `cv.warpAffine()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d596c-84ff-4b60-9fad-0c90a7b0770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation\n",
    "img = cv.imread('images/lena.jfif')\n",
    "rows, cols = img.shape[:2]\n",
    "M = cv.getRotationMatrix2D((cols // 2, rows // 2), 90, 1)\n",
    "dst = cv.warpAffine(img, M, (cols, rows))\n",
    "\n",
    "cv.imshow('rotated image', dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b67a9e-b7ff-4d75-8d42-292133a1372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation\n",
    "M = np.float32([[1, 0, 100], [0, 1, 50]])\n",
    "dst = cv.warpAffine(img, M, (cols, rows))\n",
    "\n",
    "cv.imshow('translated image', dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc336ed-002b-4151-ac3d-929d0520d732",
   "metadata": {},
   "source": [
    "## OpenCV affine transformation function\n",
    "In affine transformation, all parallel lines in the original image will still be parallel in the output image. To find the transformation matrix, we need 3 points from the input image and their corresponding locations in the output image. When it comes to the implementation part, similar to rotation, two OpenCV functions are needed: `cv.getAffineTransform()` to get the $2 \\times 3$ matrix, in which this matrix will be argument for `cv.warpAffine()`.\n",
    "```python\n",
    "# Calculates affine transform from 3 pairs of the corresponding points\n",
    "cv.getAffineTransform(src, dst)\n",
    "```\n",
    "1. First argument is coordinates of triangle vertices in the source image.\n",
    "2. Coordinates of the corresponding triangle vertices in the destination image.\n",
    "\n",
    "```python\n",
    "cv.warpAffine(src, M, dsize, ...)\n",
    "```\n",
    "1. First argument, src is the input image.\n",
    "2. M is the $2 \\times 3$ transformation matrix.\n",
    "3. dsize is the size of the output image. Usually, same as the input image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5005fe-8cfb-4c3a-b827-380c2235352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('images/chessboard.png')\n",
    "\n",
    "# change to RGB\n",
    "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "# custom affine transform\n",
    "rows, cols = img.shape[:2]\n",
    "\n",
    "pts1 = np.float32([[25, 25], [100, 25], [25, 100]])\n",
    "pts2 = np.float32([[75, 75], [150, 85], [85, 150]])\n",
    "\n",
    "for pt in pts1:\n",
    "    cv.circle(img, (int(pt[0]), int(pt[1])), 1, (0, 100, 200), -1)\n",
    "\n",
    "M = cv.getAffineTransform(pts1, pts2)\n",
    "dst = cv.warpAffine(img, M, (cols, rows))\n",
    "\n",
    "for pt in pts2:\n",
    "    cv.circle(dst, (int(pt[0]), int(pt[1])), 1, (0, 0, 255), -1)\n",
    "\n",
    "plt.subplot(121), plt.imshow(img), plt.title('input'), plt.axis(\"off\")\n",
    "plt.subplot(122), plt.imshow(dst), plt.title('output'), plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2df491-d95e-40b1-b816-9aa6cd17e540",
   "metadata": {},
   "source": [
    "## Perspective transform\n",
    "<figure>\n",
    "<img src=\"img_embed/road.jpg\" style=\"width:50%\">\n",
    "<figcaption align = \"center\"> Figure 1: Road warped by perspective projection. </figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"img_embed/billboard.jpg\" style=\"width:50%\">\n",
    "<figcaption align = \"center\"> Figure 2: Billboard warped by perspective projection. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The above figures show examples of perspective projection, which results in segments close to the camera appear longer than segments of identical length that are further away from the camera (Figure 2). In addition, parallel lines may appear to converge towards a vanishing point. \n",
    "\n",
    "Perspective transform can be an issue if we want to check if the object of interest have the right shape and size. So, this beg a question: can we \"reverse\" the perspective distortion? The answer is yes given that we know distinct feature of the object (normally corners).\n",
    "\n",
    "OpenCV does provide 2 functions to perform perspective transformation:\n",
    "- `cv.getPerspectiveTransform(src, dst, solveMethod)`\n",
    "    * src and dst are coordinates of quadrangle vertices for source and target images respectively.\n",
    "    * methods (optional) to solve the linear equations. Refer to this [link](https://docs.opencv.org/4.5.5/d2/de8/group__core__array.html#gaaf9ea5dcc392d5ae04eacb9920b9674c) for more info.\n",
    "    * The output is the transformation matrix, $M$.\n",
    "- `cv.warpPerspective(src, M, dsize)` returns the warped image.\n",
    "    * src: input image\n",
    "    * M: $3 \\times 3$ transformation matrix\n",
    "    * dsize: size of output image\n",
    "    \n",
    "To learn more about image geometric transformation, you can refer to book: **Multiple View Geometry in Computer Vision** by Richard Hartley and Andrew Zisserman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329baae1-a7d5-406d-8b66-cf0f366c7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images 3/name_card_sample.webp\")\n",
    "display_images('namecard', img, adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74018312-6ff5-40c3-9b5a-451687e6ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spicy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cda1a-4c22-40c2-8845-6849733e9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_copy = img.copy()\n",
    "pts = []\n",
    "\n",
    "def mouse(event, x, y, flags, params):\n",
    "    \"\"\"\"\"\"\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        print((x,y))\n",
    "        pts.append((x,y))\n",
    "        cv.circle(img_copy, (x,y), 2, (0,0,255), -1)\n",
    "        cv.imshow(\"img\", img_copy)\n",
    "\n",
    "cv.namedWindow(\"img\", cv.WINDOW_NORMAL)\n",
    "cv.imshow(\"img\", img_copy)\n",
    "cv.setMouseCallBack(\"img\", mouse)\n",
    "cv.waitkey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13007b-bd08-470b-bf2d-0cfc9e33ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance between points\n",
    "w1 = euclidean(pts[0], pts[1])\n",
    "w2 = euclidean(pts[2], pts[3])\n",
    "w = max(w1,w2)\n",
    "\n",
    "h1 = euclidean(pts[0], pts[1])\n",
    "h2 = euclidean(pts[2], pts[3])\n",
    "h = max(h1,h2)\n",
    "\n",
    "dst_points = np.float32([[0,0], [w-1, 0], [w-1, h-1], [0, h-1]]) #top left, top right, bottom left, bottom right\n",
    "pts_arr = np.float32(pts)\n",
    "\n",
    "M = cv.getPerspectiveTransform(pts_arr, dst_points)\n",
    "dst = cv.warpPerspective(img, M, (int(w), int(h)))\n",
    "\n",
    "display_image = (\"perspective\", dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d53c2-6e04-4398-bccc-4476c6fee2a0",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use the image \"dice.jfif\".\n",
    "\n",
    "![dice](img_embed/dice_marked.jpg \"dice\")\n",
    "\n",
    "Obtain the bird eye view of the region outlined in red with perspective transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411a356-f7ce-475c-a699-14f337d3bde7",
   "metadata": {},
   "source": [
    "# More image augmentation techniques\n",
    "* Flipping an image\n",
    "* Random cropping\n",
    "* Color jittering\n",
    "* Adding noise\n",
    "\n",
    "## Flipping an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa38328-6e4d-4363-ab03-42b62832130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_images\n",
    "\n",
    "img = cv.imread(\"images/meal.jpg\")\n",
    "img_ud = cv.flip(img, 0)  # around x-axis\n",
    "img_lr = cv.flip(img, 1)  # positive: left right\n",
    "img_lrud = cv.flip(img, -1)\n",
    "\n",
    "display_images([img, img_lr, img_ud, img_lrud], \n",
    "              (\"original\", \"left right\", \"up-down\", \"both\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553468c5-0bdc-4c52-be7e-feb41a440ea3",
   "metadata": {},
   "source": [
    "## Random cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f53441-9ec2-4365-a6d1-33e5cb153bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def randomcrop(img, scale=0.5, seed=None):\n",
    "    \"\"\"Randomly crop image. The recommended scale is [0.5, 0.95]\n",
    "    Argument: \n",
    "    ---\n",
    "    img: source image (uint8 array)\n",
    "    scale: fraction [0.5-0.95]\n",
    "    seed: random generator seed\n",
    "    \n",
    "    Return:\n",
    "    ---\n",
    "    Cropped image\"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    height, width = int(img.shape[0] * scale), int(img.shape[1] * scale)\n",
    "    x = random.randint(0, img.shape[1] - int(width))\n",
    "    y = random.randint(0, img.shape[0] - int(height))\n",
    "    cropped = img[y: y+height, x: x+width]\n",
    "    resized = cv.resize(cropped, (img.shape[1], img.shape[0]))\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c3da4-50be-464a-830c-548b717d1809",
   "metadata": {},
   "source": [
    "## Color jittering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7ed84-8025-4bd7-a58a-459a7ec2f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorjitter(img, cj_type=\"b\", seed=None):\n",
    "    \"\"\"Color jitter function\n",
    "    Argument:\n",
    "    ---\n",
    "    img: BGR source image (uint8)\n",
    "    cj_type: \n",
    "    - b: brightness\n",
    "    - s: saturation\n",
    "    - c: contrast\n",
    "    - h: hue\n",
    "    seed: random generator seed\n",
    "\n",
    "    Return:\n",
    "    ---\n",
    "    resulting image (uint8)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    if cj_type == \"b\":\n",
    "        value = random.randint(-60, 60)\n",
    "        while value == 0:\n",
    "            value = random.randint(-60, 60)\n",
    "\n",
    "        hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "        h, s, v = cv.split(hsv)\n",
    "        # cast to float\n",
    "        v_channel = np.float32(v) + value\n",
    "        v_channel = np.clip(v_channel, 0, 255).astype(\"uint8\")\n",
    "        final_hsv = cv.merge((h, s, v_channel))\n",
    "        img = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
    "        return img\n",
    "\n",
    "    if cj_type == \"s\":\n",
    "        value = random.randint(-60, 60)\n",
    "        while value == 0:\n",
    "            value = random.randint(-60, 60)\n",
    "\n",
    "        hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "        h, s, v = cv.split(hsv)\n",
    "        # cast to float\n",
    "        s_channel = np.float32(s) + value\n",
    "        s_channel = np.clip(s_channel, 0, 255).astype(\"uint8\")\n",
    "        final_hsv = cv.merge((h, s_channel, v))\n",
    "        img = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
    "        return img\n",
    "\n",
    "    if cj_type == \"c\":\n",
    "        # multiplier coefficient randomly generated centered around mean of 1\n",
    "        coef = random.gauss(1, 0.3)\n",
    "        img_float = np.float32(img) * coef\n",
    "        img_final = np.clip(img_float, 0, 255).astype(\"uint8\")\n",
    "        return img_final\n",
    "\n",
    "    if cj_type == \"h\":\n",
    "        value = random.randint(-30, 30)\n",
    "        while value == 0:\n",
    "            value = random.randint(-30, 30)\n",
    "\n",
    "        hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "        h, s, v = cv.split(hsv)\n",
    "        # cast to float\n",
    "        h_channel = np.float32(h) + value\n",
    "        h_channel = np.clip(h_channel, 0, 179).astype(\"uint8\")\n",
    "        final_hsv = cv.merge((h_channel, s, v))\n",
    "        img = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ff7d3-bbdc-431a-99ea-8bda4f8877d7",
   "metadata": {},
   "source": [
    "## Noise injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbce63a-fb07-46da-9493-a0dac648497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img, mean=0, sigma=30, seed=None):\n",
    "    \"\"\"Add Gaussian noise to image.\n",
    "    Argument:\n",
    "    ---\n",
    "    img: source image (uint8)\n",
    "    mean, sigma: parameters of Gaussian pdf, with default of 0 and 30 respectively\n",
    "    seed: random number generator seed\n",
    "    \n",
    "    Return:\n",
    "    ---\n",
    "    resulting image with noise (uint8)\"\"\"\n",
    "    img = img.astype(\"float32\")\n",
    "\n",
    "    height, width, channels = img.shape\n",
    "    if seed is not None: #set seed, everytime run get same result\n",
    "        rng = np.random.default_rng(seed)\n",
    "    else:\n",
    "        rng = np.random.default_rng(seed)\n",
    "    noise = rng.normal(loc=mean, scale=sigma, size=(height, width, channels)) #noise = rng.normal(mean, sigma, size=img.shape)\n",
    "\n",
    "    img_noise = img + noise\n",
    "    img_noise = np.clip(img_noise, 0, 255).astype(\"uint8\")\n",
    "    return img_noise\n",
    "\n",
    "    #return cv.add(img, noise, dtype = cv.CV_BU)\n",
    "    #img_wirh_noise = add_gaussian_noise(img, sigma=30)\n",
    "    #display_images([img, img_with_noise], (\"original\", \"noise\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d803081a-8272-40a0-8d84-6858fa625840",
   "metadata": {},
   "source": [
    "# Bitwise operations\n",
    "In this section, we will review four bitwise operations: *AND, OR, XOR* and *NOT*. While very basic and low level, these 4 operations are indispensible, when it comes to **masking**, **extracting specific region of an image** and **pixel-level manipulation**. \n",
    "\n",
    "Bitwise operations function in a binary manner. A given pixel is turned off if it has a value of zero and it is turned on if the pixel has a value greater than zero.\n",
    "```python\n",
    "dst = cv.bitwise_and(src1, src2[, dst[, mask]])\n",
    "```\n",
    "The first and second arguments are input array (images). Argument 'mask' is optional operation mask. `cv.bitwise_or` and `cv.bitwise_xor` method signature are similar to `cv.bitwise_and`.\n",
    "\n",
    "```python\n",
    "dst = cv.bitwise_not(src[, dst[, mask]])\n",
    "```\n",
    "The first argument is the input array, while argument 'mask' is optional operation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b090b2-2396-475a-af38-c7b4a0a56c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_images, display_image\n",
    "\n",
    "rect = np.zeros((400, 400), dtype=np.uint8)\n",
    "cv.rectangle(rect, (30, 30), (370, 370), 255, -1)\n",
    "\n",
    "circle = np.zeros((400, 400), dtype=np.uint8)\n",
    "cv.circle(circle, (200, 200), 200, 255, -1)\n",
    "display_images([rect, circle], (\"rectangle\", \"circle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933700c-ef4c-49e0-b29a-ce6813c7391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND operation\n",
    "res_AND = cv.bitwise_and(rect, circle)\n",
    "display_image('AND operator result', res_AND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6b637-c508-439b-86f7-ccbbef9e589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR operation\n",
    "res_OR = cv.bitwise_or(rect, circle)\n",
    "display_image(\"OR operator result\", res_OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dba890-f8b8-4835-be24-8f3edb60bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR operation\n",
    "res_XOR = cv.bitwise_xor(rect, circle)\n",
    "display_image('XOR operator result', res_XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bb56d-bd1a-4899-b274-c8eb4ce753b6",
   "metadata": {},
   "source": [
    "**Exercise**  \n",
    "Turn the white rectangle on dark background to black rectangle on white background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1f65c-9301-427b-ac22-66c945337b9d",
   "metadata": {},
   "source": [
    "# Weekly activity\n",
    "1. Rotate image by 45 degrees without cropping the sides of the image. (Hint: There are 2 strategies to tackle these problems). Use _\"lena.jfif\"_ as the input image.\n",
    "    - Use external libraries `imutils`.  \n",
    "    - Modify the transformation matrix.\n",
    "2. Use the images with titles: _\"flower.jfif\"_ and _\"native-bee.png\"_. I want to put flower above an image. If I add two images, it will change color. If I blend it, I get a transparent effect. But I want it to be opaque. If it was a rectangular region, we could use the ROI as we did in the previous section. But flower is not a rectangular region. This is where bitwise operations, like AND, OR, NOT and XOR really come in handy. The associated functions are `cv.bitwise_and()`, `cv.bitwise_or()` and `cv.bitwise_not()`. You need to use `cv.threshold` function to segment the flower. Please refer to [online documentation](https://docs.opencv.org/4.x/d0/d86/tutorial_py_image_arithmetics.html) for more info. The result should resemble the following:  \n",
    "![bee and flowers](img_embed/activity3.PNG \"bee_flower\")\n",
    "3. Write a function that randomly crop the central region of an image. The method signature should be as shown in the following:\n",
    "```\n",
    "random_center_crop(image, min_crop_ratio, max_crop_ratio)\n",
    "```\n",
    "\n",
    "4. Aside from Gaussian noise, name another common type of noise. Write the code to demonstrate how the noise can be included in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c13986-2445-49b7-aac7-1eb9f94cd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "#Method 1\n",
    "import cv2 as cv\n",
    "import imutils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = cv.imread('images/lena.jfif')\n",
    "\n",
    "# Rotate the image by 45 degrees without cropping\n",
    "rotated_image = imutils.rotate_bound(image, 45)\n",
    "\n",
    "# Display the original and rotated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Rotated Image (imutils)\")\n",
    "plt.imshow(cv.cvtColor(rotated_image, cv.COLOR_BGR2RGB))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b5598-c7e0-4bf0-b996-3f4d9fc208f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "#Method 2\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = cv.imread('images/lena.jfif')\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# Calculate the center of the image\n",
    "center = (w // 2, h // 2)\n",
    "\n",
    "# Calculate the rotation matrix\n",
    "angle = 45\n",
    "M = cv.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "# Calculate the new bounding dimensions of the image\n",
    "cos = np.abs(M[0, 0])\n",
    "sin = np.abs(M[0, 1])\n",
    "new_w = int((h * sin) + (w * cos))\n",
    "new_h = int((h * cos) + (w * sin))\n",
    "\n",
    "# Adjust the rotation matrix to take into account the translation\n",
    "M[0, 2] += (new_w / 2) - center[0]\n",
    "M[1, 2] += (new_h / 2) - center[1]\n",
    "\n",
    "# Perform the actual rotation and get the resulting image\n",
    "rotated_image = cv.warpAffine(image, M, (new_w, new_h))\n",
    "\n",
    "# Display the original and rotated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Rotated Image (Manual Matrix)\")\n",
    "plt.imshow(cv.cvtColor(rotated_image, cv.COLOR_BGR2RGB))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e2355-9dbd-48e5-a38c-cdf59b25fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the images\n",
    "flower = cv.imread('images 3/flower.jfif')\n",
    "bee = cv.imread('images 3/native-bee.png')\n",
    "\n",
    "# Convert flower to grayscale\n",
    "gray_flower = cv.cvtColor(flower, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a binary mask of the flower\n",
    "_, mask = cv.threshold(gray_flower, 70, 255, cv.THRESH_BINARY)\n",
    "\n",
    "# Create the inverse mask\n",
    "mask_inv = cv.bitwise_not(mask)\n",
    "\n",
    "# Get the dimensions of the flower image\n",
    "flower_height, flower_width = flower.shape[:2]\n",
    "\n",
    "# Define the region of interest (ROI) in the bee image where the flower will be placed\n",
    "x_offset = 40 \n",
    "y_offset = 10   \n",
    "roi = bee[y_offset:y_offset + flower_height, x_offset:x_offset + flower_width]\n",
    "\n",
    "# Black-out the area of the flower in the bee image using mask_inv\n",
    "bee_bg = cv.bitwise_and(roi, roi, mask=mask_inv)\n",
    "\n",
    "# Take only region of the flower from the flower image using the mask\n",
    "flower_fg = cv.bitwise_and(flower, flower, mask=mask)\n",
    "\n",
    "# Add the flower_fg and bee_bg to get the final image in the ROI\n",
    "dst = cv.add(bee_bg, flower_fg)\n",
    "\n",
    "# Place the combined image back into the original bee image\n",
    "bee[y_offset:y_offset + flower_height, x_offset:x_offset + flower_width] = dst\n",
    "\n",
    "# Display the result\n",
    "cv.imshow('flower_bee', bee)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dfb1d4-3952-4cb9-8882-fd0e77b69a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "import random\n",
    "\n",
    "def random_center_crop(image, min_crop_ratio, max_crop_ratio):\n",
    "    h, w = image.shape[:2]\n",
    "    crop_ratio = random.uniform(min_crop_ratio, max_crop_ratio)\n",
    "    crop_h = int(h * crop_ratio)\n",
    "    crop_w = int(w * crop_ratio)\n",
    "    \n",
    "    start_y = (h - crop_h) // 2\n",
    "    start_x = (w - crop_w) // 2\n",
    "    \n",
    "    cropped_image = image[start_y:start_y + crop_h, start_x:start_x + crop_w]\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc870fe5-bd2d-4a57-9591-0c49e46c3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "def salt_and_pepper_noise(image, salt_prob, pepper_prob):\n",
    "    noisy_image = image.copy()\n",
    "    num_salt = np.ceil(salt_prob * image.size)\n",
    "    num_pepper = np.ceil(pepper_prob * image.size)\n",
    "    \n",
    "    # Add salt noise (white pixels)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape]\n",
    "    noisy_image[coords[0], coords[1], :] = 1\n",
    "    \n",
    "    # Add pepper noise (black pixels)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.shape]\n",
    "    noisy_image[coords[0], coords[1], :] = 0\n",
    "    \n",
    "    return noisy_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
